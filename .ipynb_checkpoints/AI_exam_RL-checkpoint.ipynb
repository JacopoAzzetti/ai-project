{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c707f35e",
   "metadata": {},
   "source": [
    "# AI Exam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233976dd",
   "metadata": {},
   "source": [
    "Consider the following environment:\n",
    "\n",
    "<img src=\"images/road_env.jpg\" style=\"zoom: 40%;\"/>\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and must reach the goal in cell $(8,6)$. The agent can move in the four directions (except when a wall is present), and for each step taken the agent receives a negative reward.\n",
    "In cells representing roads with intersections, the agent must wait for the traffic light to turn green before proceeding. At busy intersections (indicated by two traffic lights in the same cell), the agent will have to wait a long time to cross the intersection. This means that if the agent tries to move to another cell, the action may not succeed, causing the agent to remain in the same cell for an unknown amount of time.\n",
    "\n",
    "Assume that you do not have access to the motion model and to reward and that the problem is undiscounted (i.e., $\\gamma$=1), find a policy for the environment reported above with a suitable algorithm of your choice.\n",
    "\n",
    "\n",
    "<span style=\"color:green\">Mi sembra sia tutto ok sia per sarsa sia per q-learning (implementate con softmax ed $\\epsilon$-greedy rispettivamente), non sono sicura di come implementare la funzione di check per√≤ dato che le policy potrebbero essere differenti..</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5be1e677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['S' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['W' 'Ts' 'R' 'R' 'R' 'R' 'Tl' 'R' 'R']\n",
      " ['W' 'R' 'W' 'W' 'W' 'W' 'R' 'W' 'W']\n",
      " ['R' 'Ts' 'R' 'Ts' 'R' 'R' 'Ts' 'W' 'W']\n",
      " ['W' 'W' 'W' 'R' 'W' 'W' 'R' 'Ts' 'R']\n",
      " ['W' 'R' 'R' 'Tl' 'W' 'W' 'W' 'R' 'W']\n",
      " ['W' 'R' 'W' 'R' 'Ts' 'R' 'R' 'Tl' 'R']\n",
      " ['W' 'R' 'W' 'W' 'R' 'W' 'W' 'R' 'W']\n",
      " ['R' 'Ts' 'R' 'R' 'Tl' 'R' 'G' 'Ts' 'R']]\n",
      "\n",
      "Actions encoding:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Cell type of start state:  S\n",
      "Cell type of goal state:  G\n",
      "Cell type of cell (1, 6):  Tl\n",
      "Cell type of cell (1, 1):  Ts\n"
     ]
    }
   ],
   "source": [
    "import os, sys \n",
    "\n",
    "module_path = os.path.abspath(os.path.join('tools'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import gym, envs\n",
    "from utils.ai_lab_functions import *\n",
    "import numpy as np\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "env_name = 'RoadEnv-v0'\n",
    "env = gym.make(env_name)\n",
    "\n",
    "env.render()\n",
    "\n",
    "print(\"\\nActions encoding: \", env.actions)\n",
    "\n",
    "# Remember that you can know the type of a cell whenever you need by accessing the grid element of the environment:\n",
    "print(\"Cell type of start state: \",env.grid[env.startstate])\n",
    "print(\"Cell type of goal state: \",env.grid[env.goalstate])\n",
    "state = 15 # a very busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])\n",
    "state = 10 # a less busy intersection\n",
    "print(f\"Cell type of cell {env.state_to_pos(state)}: \",env.grid[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8fa64cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(q, state, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.choice(q.shape[1])\n",
    "    return q[state].argmax()\n",
    "\n",
    "\n",
    "def softmax(q, state, temp):\n",
    "    e = np.exp(q[state] / temp)\n",
    "    return np.random.choice(q.shape[1], p=e / e.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39d99a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()\n",
    "        el = 0\n",
    "        \n",
    "        while True:\n",
    "            action = expl_func(q, state, expl_param)  \n",
    "            next_state, reward, done, _ = environment.step(action)  \n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state].max() - q[state, action])\n",
    "            rews[i] += reward\n",
    "            el += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state  # Update state\n",
    "            \n",
    "        lengths[i] = el\n",
    "        \n",
    "    policy = q.argmax(axis=1)\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54345f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(environment, episodes, alpha, gamma, expl_func, expl_param):\n",
    "    q = np.zeros((environment.observation_space.n, environment.action_space.n))  # Q(s, a)\n",
    "    rews = np.zeros(episodes)\n",
    "    lengths = np.zeros(episodes)\n",
    "    for i in range(episodes):\n",
    "        state = environment.reset()  # Reset the environment\n",
    "        action = expl_func(q, state, expl_param)  # Select first action\n",
    "        el = 0\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done, _ = environment.step(action)  # Execute a step\n",
    "            action_next = expl_func(q, next_state, expl_param)  # Select next action\n",
    "            q[state, action] += alpha * (reward + gamma * q[next_state, action_next] - q[state, action])  # Temporal difference\n",
    "            rews[i] += reward\n",
    "            el += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state  # Update state\n",
    "            action = action_next  # Update action\n",
    "            \n",
    "        lengths[i] = el\n",
    "        \n",
    "    policy = q.argmax(axis=1) # q.argmax(axis=1) automatically extract the policy from the q table\n",
    "    return policy, rews, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa046cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.4217s\n",
      "[['R' 'D' 'L' 'L' 'L' 'L' 'U' 'L' 'L']\n",
      " ['L' 'D' 'L' 'D' 'R' 'R' 'D' 'D' 'U']\n",
      " ['L' 'D' 'L' 'L' 'L' 'L' 'D' 'L' 'L']\n",
      " ['R' 'R' 'R' 'D' 'L' 'U' 'D' 'L' 'L']\n",
      " ['L' 'L' 'L' 'D' 'L' 'L' 'R' 'D' 'R']\n",
      " ['L' 'R' 'R' 'D' 'L' 'L' 'L' 'D' 'L']\n",
      " ['L' 'U' 'L' 'R' 'D' 'L' 'R' 'D' 'R']\n",
      " ['L' 'D' 'L' 'L' 'D' 'L' 'L' 'D' 'L']\n",
      " ['L' 'U' 'R' 'R' 'R' 'R' 'L' 'L' 'L']]\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "policy, rewards, lengths = q_learning(env, episodes, alpha, gamma, epsilon_greedy, epsilon)\n",
    "print(f\"Execution time: {round(timer() - t, 4)}s\") \n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.shape))\n",
    "print(policy_render)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "128c157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 47.1699s\n",
      "[['R' 'D' 'L' 'L' 'L' 'L' 'D' 'L' 'L']\n",
      " ['L' 'R' 'R' 'R' 'R' 'R' 'D' 'L' 'L']\n",
      " ['L' 'D' 'L' 'L' 'L' 'L' 'D' 'L' 'L']\n",
      " ['R' 'R' 'R' 'D' 'L' 'R' 'D' 'L' 'L']\n",
      " ['L' 'L' 'L' 'D' 'L' 'L' 'R' 'D' 'D']\n",
      " ['L' 'R' 'D' 'D' 'L' 'L' 'L' 'D' 'L']\n",
      " ['L' 'D' 'L' 'R' 'D' 'R' 'R' 'D' 'L']\n",
      " ['L' 'D' 'L' 'L' 'D' 'L' 'L' 'D' 'L']\n",
      " ['U' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R']]\n"
     ]
    }
   ],
   "source": [
    "# Learning parameters\n",
    "episodes = 500\n",
    "alpha = .3\n",
    "gamma = .9\n",
    "epsilon = .1\n",
    "\n",
    "t = timer()\n",
    "\n",
    "policy, rewards, lengths = sarsa(env, episodes, alpha, gamma, softmax, epsilon)\n",
    "print(f\"Execution time: {round(timer() - t, 4)}s\") \n",
    "policy_render = np.vectorize(env.actions.get)(policy.reshape(env.shape))\n",
    "print(policy_render)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "703075dc036e8ebc3a027aec30cd295176a007527fa40434b7705e84e779ac0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
